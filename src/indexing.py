from llama_index.core import (  # type: ignore
    VectorStoreIndex,
    SummaryIndex,
    Document,
    Settings,
)
from llama_index.core.schema import IndexNode, TextNode  # type: ignore
from llama_index.embeddings.huggingface import HuggingFaceEmbedding  # type: ignore
from typing import Dict, Any, List, Optional, Tuple
import os
from dotenv import load_dotenv

load_dotenv()

from llama_index.core.llms.llm import LLM
from llama_index.llms.litellm import LiteLLM
from pydantic import Field
import json
import requests
import os

os.environ["GEMINI_API_KEY"] = os.getenv("GEMINI_API_KEY")


# Simple response classes
class SimpleCompletionResponse:
    def __init__(self, text: str):
        self.text = text


class SimpleChatMessage:
    def __init__(self, role: str, content: str):
        self.role = role
        self.content = content


class SimpleChatResponse:
    def __init__(self, message):
        self.message = message


class OpenRouter(LLM):
    """
    Custom OpenRouter LLM for LlamaIndex.
    """

    openrouter_api_key: str = Field(description="OpenRouter API key")
    model: str = Field(description="Model name to use")
    provider_order: Optional[List[str]] = Field(
        default=None, description="Provider order preference"
    )

    def __init__(
        self,
        openrouter_api_key: str,
        model: str,
        provider_order: Optional[List[str]] = None,
        **kwargs,
    ):
        super().__init__(
            openrouter_api_key=openrouter_api_key,
            model=model,
            provider_order=provider_order,
            **kwargs,
        )

    def _call_api(self, messages: List[Dict], **kwargs) -> str:
        """Internal method to call OpenRouter API."""
        headers = {
            "Authorization": f"Bearer {self.openrouter_api_key}",
            "Content-Type": "application/json",
        }

        data = {
            "model": self.model,
            "messages": messages,
        }

        if self.provider_order:
            data["provider"] = {"order": self.provider_order}

        try:
            response = requests.post(
                url="https://openrouter.ai/api/v1/chat/completions",
                headers=headers,
                data=json.dumps(data),
                timeout=60,
            )
            response.raise_for_status()
            result = response.json()
            return result["choices"][0]["message"]["content"]
        except Exception as e:
            print(f"OpenRouter API error: {e}")
            return f"Error calling OpenRouter API: {e}"

    def complete(self, prompt: str, **kwargs):
        """Calls OpenRouter API and returns the completion response."""
        messages = [{"role": "user", "content": prompt}]
        content = self._call_api(messages, **kwargs)
        return SimpleCompletionResponse(text=content)

    def chat(self, messages, **kwargs):
        """Chat interface."""
        # Convert messages to proper format if needed
        if hasattr(messages, "messages"):
            chat_messages = [
                {"role": msg.role, "content": msg.content} for msg in messages.messages
            ]
        else:
            chat_messages = messages
        content = self._call_api(chat_messages, **kwargs)
        return SimpleChatResponse(
            message=SimpleChatMessage(role="assistant", content=content)
        )

    def stream_complete(self, prompt: str, **kwargs):
        """Stream completion - simplified implementation."""
        result = self.complete(prompt, **kwargs)
        yield result

    def stream_chat(self, messages, **kwargs):
        """Stream chat - simplified implementation."""
        result = self.chat(messages, **kwargs)
        yield result

    async def acomplete(self, prompt: str, **kwargs):
        """Async completion - falls back to sync."""
        return self.complete(prompt, **kwargs)

    async def achat(self, messages, **kwargs):
        """Async chat - falls back to sync."""
        return self.chat(messages, **kwargs)

    async def astream_complete(self, prompt: str, **kwargs):
        """Async stream completion - falls back to sync."""
        for chunk in self.stream_complete(prompt, **kwargs):
            yield chunk

    async def astream_chat(self, messages, **kwargs):
        """Async stream chat - falls back to sync."""
        for chunk in self.stream_chat(messages, **kwargs):
            yield chunk

    @property
    def metadata(self):
        """Return metadata about the model."""

        class LLMMetadata:
            def __init__(self, model_name):
                self.context_window = 4096  # Default context window
                self.num_output = 512  # Default max output tokens
                self.model_name = model_name

        return LLMMetadata(self.model)


def setup_models():
    """C·∫•u h√¨nh c√°c m√¥ h√¨nh LLM v√† Embedding."""
    # Settings.llm = OpenRouter(
    #     openrouter_api_key=os.getenv("API_KEY"),
    #     model=os.getenv("MODEL_NAME"),
    #     provider_order=["DeepInfra", "Together"],
    # )
    Settings.llm = LiteLLM(
        model="gemini/gemini-2.5-flash-preview-04-17",  # Model d√πng LiteLLM wrapper
        api_key=os.getenv("GEMINI_API_KEY"),
        context_window=8192,  # S·ªë token c√≥ th·ªÉ t√πy ch·ªânh (Gemini Pro h·ªó tr·ª£ l·ªõn)
        max_tokens=1024,
    )
    Settings.embed_model = HuggingFaceEmbedding(
        model_name="dangvantuan/vietnamese-document-embedding", trust_remote_code=True
    )
    print("Models setup complete.")


def build_hierarchical_index(law_tree: dict):
    """
    X√¢y d·ª±ng ch·ªâ m·ª•c ph√¢n c·∫•p theo paper HiRAG v·ªõi bottom-up summarization.

    Hierarchy: Kho·∫£n (Clause) ‚Üí ƒêi·ªÅu (Article) ‚Üí M·ª•c (Section) ‚Üí Ch∆∞∆°ng (Chapter) ‚Üí Ph·∫ßn (Part)

    Args:
        law_tree: C·∫•u tr√∫c c√¢y d·ªØ li·ªáu lu·∫≠t ƒë√£ parse

    Returns:
        tuple: (top_index, child_query_engines)
    """
    print("üî® Building hierarchical index with bottom-up summarization...")

    all_part_nodes = []
    child_query_engines = {}

    # L·∫•y metadata
    title = law_tree.get("metadata", {}).get("title", "VƒÉn b·∫£n lu·∫≠t")

    # Duy·ªát qua t·ª´ng PH·∫¶N
    for part_title, part_content in law_tree["content"].items():
        print(f"\nüìö Processing Part: {part_title}")

        part_chapter_nodes = []

        # Duy·ªát qua t·ª´ng CH∆Ø∆†NG trong Ph·∫ßn
        for chapter_title, chapter_content in part_content.items():
            print(f"  üìñ Processing Chapter: {chapter_title}")

            chapter_nodes = []

            # X·ª≠ l√Ω n·ªôi dung ch∆∞∆°ng - c√≥ th·ªÉ l√† ƒêi·ªÅu tr·ª±c ti·∫øp ho·∫∑c M·ª•c
            for item_key, item_content in chapter_content.items():

                # Ki·ªÉm tra xem c√≥ ph·∫£i l√† M·ª§C kh√¥ng
                if item_key.startswith("M·ª•c"):
                    # Case: Ch∆∞∆°ng -> M·ª•c -> ƒêi·ªÅu -> Kho·∫£n
                    section_title = item_key
                    print(f"    üìë Processing Section: {section_title}")

                    section_article_nodes = []

                    # Duy·ªát qua c√°c ƒêi·ªÅu trong M·ª•c
                    for article_title, article_clauses in item_content.items():
                        article_node, article_query_engine = build_article_index(
                            article_title,
                            article_clauses,
                            f"{section_title} - {article_title}",
                        )
                        if article_node:
                            section_article_nodes.append(article_node)
                            child_query_engines[article_title] = article_query_engine

                    # T·∫°o index cho M·ª•c t·ª´ c√°c ƒêi·ªÅu
                    if section_article_nodes:
                        section_index = VectorStoreIndex(section_article_nodes)
                        section_summary = summarize_nodes(
                            section_article_nodes,
                            f"T√≥m t·∫Øt n·ªôi dung c·ªßa {section_title}",
                            "section",
                        )

                        section_node = IndexNode(
                            text=f"{section_title}: {section_summary}",
                            index_id=section_title,
                        )
                        chapter_nodes.append(section_node)
                        child_query_engines[section_title] = (
                            section_index.as_query_engine()
                        )

                elif item_key.startswith("ƒêi·ªÅu"):
                    # Case: Ch∆∞∆°ng -> ƒêi·ªÅu -> Kho·∫£n (kh√¥ng c√≥ M·ª•c)
                    article_title = item_key
                    article_clauses = item_content

                    article_node, article_query_engine = build_article_index(
                        article_title,
                        article_clauses,
                        f"{chapter_title} - {article_title}",
                    )
                    if article_node:
                        chapter_nodes.append(article_node)
                        child_query_engines[article_title] = article_query_engine

            # T·∫°o index cho Ch∆∞∆°ng t·ª´ c√°c ƒêi·ªÅu/M·ª•c
            if chapter_nodes:
                chapter_index = VectorStoreIndex(chapter_nodes)
                chapter_summary = summarize_nodes(
                    chapter_nodes, f"T√≥m t·∫Øt n·ªôi dung c·ªßa {chapter_title}", "chapter"
                )

                chapter_node = IndexNode(
                    text=f"{chapter_title}: {chapter_summary}", index_id=chapter_title
                )
                part_chapter_nodes.append(chapter_node)
                child_query_engines[chapter_title] = chapter_index.as_query_engine()

        # T·∫°o index cho Ph·∫ßn t·ª´ c√°c Ch∆∞∆°ng
        if part_chapter_nodes:
            part_index = VectorStoreIndex(part_chapter_nodes)
            part_summary = summarize_nodes(
                part_chapter_nodes, f"T√≥m t·∫Øt n·ªôi dung c·ªßa {part_title}", "part"
            )

            part_node = IndexNode(
                text=f"{part_title}: {part_summary}", index_id=part_title
            )
            all_part_nodes.append(part_node)
            child_query_engines[part_title] = part_index.as_query_engine()

    # T·∫°o top-level index t·ª´ c√°c Ph·∫ßn
    if not all_part_nodes:
        raise ValueError(
            "Kh√¥ng c√≥ d·ªØ li·ªáu ƒë·ªÉ t·∫°o index. Ki·ªÉm tra l·∫°i c·∫•u tr√∫c law_tree."
        )

    top_index = VectorStoreIndex(all_part_nodes)

    print(f"\n‚úÖ Hierarchical index built successfully!")
    print(f"   üìä Total Parts: {len(all_part_nodes)}")
    print(f"   üîß Total Query Engines: {len(child_query_engines)}")

    return top_index, child_query_engines


def build_article_index(
    article_title: str, article_clauses: Dict[str, str], context: str = ""
) -> Tuple[IndexNode, Any]:
    """
    X√¢y d·ª±ng index cho m·ªôt ƒêi·ªÅu t·ª´ c√°c Kho·∫£n.

    Args:
        article_title: Ti√™u ƒë·ªÅ ƒëi·ªÅu (vd: "ƒêi·ªÅu 1. Ph·∫°m vi ƒëi·ªÅu ch·ªânh")
        article_clauses: Dict c√°c kho·∫£n {kho·∫£n_key: n·ªôi_dung}
        context: Context ƒë·ªÉ debug

    Returns:
        tuple: (article_index_node, article_query_engine)
    """
    if not article_clauses:
        print(f"    ‚ö†Ô∏è No clauses found for {article_title}")
        return None, None

    # 1. T·∫°o TextNode t·ª´ m·ªói Kho·∫£n (leaf nodes)
    clause_nodes = []
    for clause_key, clause_content in article_clauses.items():
        if clause_content and clause_content.strip():
            clause_node = TextNode(
                text=f"{article_title} - {clause_key}: {clause_content.strip()}",
                metadata={
                    "article": article_title,
                    "clause": clause_key,
                    "level": "clause",
                    "context": context,
                },
            )
            clause_nodes.append(clause_node)

    if not clause_nodes:
        print(f"    ‚ö†Ô∏è No valid clause nodes for {article_title}")
        return None, None

    # 2. T·∫°o VectorStoreIndex t·ª´ c√°c Kho·∫£n
    clause_index = VectorStoreIndex(clause_nodes)

    # 3. T√≥m t·∫Øt c√°c Kho·∫£n th√†nh summary c·ªßa ƒêi·ªÅu (bottom-up)
    article_summary = summarize_nodes(
        clause_nodes, f"T√≥m t·∫Øt n·ªôi dung ch√≠nh c·ªßa {article_title}", "article"
    )

    # 4. T·∫°o IndexNode cho ƒêi·ªÅu
    article_node = IndexNode(
        text=f"{article_title}: {article_summary}",
        index_id=article_title,
        metadata={
            "level": "article",
            "num_clauses": len(clause_nodes),
            "context": context,
        },
    )

    print(f"    ‚úÖ Built article index: {article_title} ({len(clause_nodes)} clauses)")

    return article_node, clause_index.as_query_engine()


def summarize_nodes(nodes: List, summary_prompt_prefix: str, level: str) -> str:
    """
    T√≥m t·∫Øt n·ªôi dung t·ª´ list of nodes theo paper HiRAG.

    Args:
        nodes: List TextNode ho·∫∑c IndexNode c·∫ßn t√≥m t·∫Øt
        summary_prompt_prefix: Prefix cho prompt t√≥m t·∫Øt
        level: Level hi·ªán t·∫°i (clause, article, section, chapter, part)

    Returns:
        str: N·ªôi dung t√≥m t·∫Øt
    """
    if not nodes:
        return "Kh√¥ng c√≥ n·ªôi dung."

    # L·∫•y text t·ª´ c√°c nodes
    node_texts = []
    for node in nodes:
        if hasattr(node, "text"):
            node_texts.append(node.text)
        elif hasattr(node, "get_content"):
            node_texts.append(node.get_content())

    if not node_texts:
        return "Kh√¥ng c√≥ n·ªôi dung h·ª£p l·ªá."

    # T·∫°o prompt t√≥m t·∫Øt ph√π h·ª£p v·ªõi t·ª´ng level
    combined_text = "\n".join(node_texts)

    # Gi·ªõi h·∫°n ƒë·ªô d√†i input cho LLM
    max_chars = 8000  # Gi·ªõi h·∫°n ƒë·ªÉ tr√°nh token limit
    if len(combined_text) > max_chars:
        combined_text = combined_text[:max_chars] + "..."

    summary_prompt = f"""
{summary_prompt_prefix} d·ª±a tr√™n c√°c n·ªôi dung sau:

{combined_text}

H√£y t√≥m t·∫Øt ng·∫Øn g·ªçn v√† s√∫c t√≠ch theo y√™u c·∫ßu sau:
- N√™u ƒë∆∞·ª£c √Ω ch√≠nh v√† ƒëi·ªÉm quan tr·ªçng nh·∫•t
- S·ª≠ d·ª•ng ng√¥n ng·ªØ ph√°p l√Ω ch√≠nh x√°c
- ƒê·ªô d√†i kh√¥ng qu√° 200 t·ª´
- Gi·ªØ nguy√™n c√°c thu·∫≠t ng·ªØ ph√°p lu·∫≠t quan tr·ªçng

T√≥m t·∫Øt:"""

    try:
        summary_response = Settings.llm.complete(summary_prompt)
        summary = summary_response.text.strip()

        if not summary or len(summary) < 10:
            # Fallback: l·∫•y ph·∫ßn ƒë·∫ßu c·ªßa combined_text
            summary = (
                combined_text[:300] + "..."
                if len(combined_text) > 300
                else combined_text
            )

        print(
            f"    üìù Summarized {level} ({len(node_texts)} items) -> {len(summary)} chars"
        )
        return summary

    except Exception as e:
        print(f"    ‚ö†Ô∏è Summarization failed for {level}: {e}")
        # Fallback: l·∫•y ph·∫ßn ƒë·∫ßu c·ªßa combined_text
        fallback_summary = (
            combined_text[:300] + "..." if len(combined_text) > 300 else combined_text
        )
        return fallback_summary


def call_llm(prompt: str) -> str:
    from openai import OpenAI

    client = OpenAI(
        base_url="https://openrouter.ai/api/v1",
        api_key=os.getenv("API_KEY"),
    )

    completion = client.chat.completions.create(
        extra_headers={
            "HTTP-Referer": "<YOUR_SITE_URL>",  # Optional. Site URL for rankings on openrouter.ai.
            "X-Title": "<YOUR_SITE_NAME>",  # Optional. Site title for rankings on openrouter.ai.
        },
        extra_body={},
        model="deepseek/deepseek-r1-0528:free",
        messages=[{"role": "user", "content": prompt}],
    )
    print(completion.choices[0].message.content)
    return completion.choices[0].message.content
